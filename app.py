from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import UnstructuredURLLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os


load_dotenv()
DEV_MODE = os.getenv("DEV_MODE", "true").lower() == "true"

llm_model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-lite')

url = ['https://en.wikipedia.org/wiki/FIFA_World_Cup']
loader = UnstructuredURLLoader(urls=url)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = []
for doc in documents:
    chunks.extend(text_splitter.split_text(doc.page_content))

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

if DEV_MODE:
    vector_store = Chroma.from_texts(
        texts=chunks,
        embedding=embedding_model,
        collection_name="qa_db"
    )
else:
    vector_store = Chroma.from_texts(
        texts=chunks,
        embedding=embedding_model,
        collection_name="qa_db",
        persist_directory="chroma_db"
    )
    vector_store.persist()


user_query = "explain how the fifa tournament works? "
embedded_user_query = embedding_model.embed_query(user_query)
context = vector_store.similarity_search_by_vector(
    embedding=embedded_user_query,
    k=5
)


context_text= "\n\n".join([doc.page_content for doc in context])

prompt = PromptTemplate(
    # template="you are a helpful web reader assistant. Use the following context to answer the question, and also you may include your own knowledge base to answer but mention that it is generated by you and not from the text.\n\nContext: {context}\n\nQuestion: {query}\n\nAnswer:",
    template="you are a helpful web reader assistant. Use the following context to answer the question.\n\nContext: {context}\n\nQuestion: {query}\n\nAnswer:",

    input_variables=["context", "query"]
)

final_prompt = prompt.format(
    context=context_text,
    query=user_query
)


print("generating response...")

response = llm_model.invoke(final_prompt)

print(response.content)